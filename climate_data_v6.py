# -*- coding: utf-8 -*-
"""Climate_data_v6

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1adbvBQ542f8U-tqpIlxtH9csUOp8QJpn
"""

# Install necessary packages
!pip install -q xarray netCDF4 cdsapi

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
from datetime import datetime
import requests
from scipy import stats
import xarray as xr
import netCDF4 as nc
from google.colab import files
from google.colab import drive

print("Libraries imported successfully!")

# Mount Google Drive
drive.mount('/content/drive', force_remount=True)

def upload_glacier_sites(file_path):
    sites_df = pd.read_csv(file_path)
    # Rename the first column to 'site_name'
    sites_df = sites_df.rename(columns={sites_df.columns[0]: 'site_name'})
    # Access latitude and longitude by column index (assuming they are in columns 1 and 2)
    sites_df = sites_df.rename(columns={sites_df.columns[1]: 'latitude', sites_df.columns[2]: 'longitude'})
    return sites_df

def setup_cds_api():
    """Setup Climate Data Store (CDS) API credentials"""
    # Install the CDS API client if needed
    import os

    # Check for API key file
    cds_key_file = os.path.expanduser('~/.cdsapirc')
    if os.path.exists(cds_key_file):
        print("CDS API configuration file already exists.")
        # Force recreation
        recreate = input("Would you like to recreate it? (y/n): ").lower() == 'y'
        if not recreate:
            return True

    print("\nCDS API configuration")
    print("Please register at https://cds.climate.copernicus.eu/user/register")
    print("Then go to https://cds.climate.copernicus.eu/api-how-to and copy your API key")

    url = input("Enter CDS API URL (default: https://cds.climate.copernicus.eu/api/v2): ")
    if not url:
        url = "https://cds.climate.copernicus.eu/api/v2"

    key = input("Enter your CDS API key: ")

    with open(cds_key_file, 'w') as f:
        f.write(f"url: {url}\n")
        f.write(f"key: {key}\n")

    os.chmod(cds_key_file, 0o600)
    print(f"CDS API configuration saved to {cds_key_file}")
    return True

def get_noaa_climate_data(sites_df, start_year=1970, end_year=None, timeout=30):
    if end_year is None:
        end_year = datetime.now().year

    token = input("Enter your NOAA API token: ")
    # Correct base URL for NOAA's Climate Data Online (CDO) API
    base_url = "https://www.ncdc.noaa.gov/cdo-web/api/v2"
    headers = {'token': token}
    results = {}

    for i, site in sites_df.iterrows():
        site_name = site['site_name']
        lat = site['latitude']
        lon = site['longitude']

        print(f"Processing site: {site_name} ({lat}, {lon})")

        # First, find stations near the coordinates
        station_url = f"{base_url}/stations"
        params = {
            'extent': f"{lat-1},{lon-1},{lat+1},{lon+1}",
            'limit': 1000
        }

        try:
            # Adding timeout parameter to prevent hanging
            response = requests.get(station_url, headers=headers, params=params, timeout=timeout)
            response.raise_for_status()  # Raise exception for bad responses

            stations = response.json()
            if 'results' not in stations or len(stations['results']) == 0:
                print(f"No stations found near {site_name}")
                continue

            closest_station = min(
                stations['results'],
                key=lambda s: ((lat - float(s['latitude']))**2 + (lon - float(s['longitude']))**2)**0.5
            )
            station_id = closest_station['id']
            station_distance = ((lat - float(closest_station['latitude']))**2 +
                               (lon - float(closest_station['longitude']))**2)**0.5
            print(f"  Using station: {closest_station['name']} (ID: {station_id})")
            print(f"  Station coordinates: {closest_station['latitude']}, {closest_station['longitude']}")
            print(f"  Distance from site: {station_distance:.2f} degrees")

            # Create a container for this site's data
            site_data = []

            # Fetch data for each year (may need to do this in chunks due to API limitations)
            for year in range(start_year, end_year + 1):
                data_url = f"{base_url}/data"
                data_params = {
                    'datasetid': 'GHCND',  # Global Historical Climatology Network Daily
                    'stationid': station_id,
                    'startdate': f"{year}-01-01",
                    'enddate': f"{year}-12-31",
                    'datatypeid': 'PRCP, TMAX, TMIN, AWND, SNOW, SNWD, ACSH',
                    'units': 'metric',
                    'limit': 1000
                }

                try:
                    # Adding timeout parameter to prevent hanging
                    response = requests.get(data_url, headers=headers, params=data_params, timeout=timeout)
                    response.raise_for_status()

                    data = response.json()
                    if 'results' in data and data['results']:
                        print(f"  Year {year}: {len(data['results'])} records")
                        site_data.extend(data['results'])
                    else:
                        print(f"  Year {year}: No data found")

                except requests.exceptions.RequestException as e:
                    print(f"  Error fetching data for {site_name}, year {year}: {e}")
                    continue

            if site_data:
                # Convert to DataFrame for easier analysis
                df = pd.DataFrame(site_data)
                results[site_name] = df
                print(f"  Total records for {site_name}: {len(df)}")

                # Show a preview of the data structure
                print("  Data columns:")
                for col in df.columns:
                    print(f"    - {col}: {df[col].dtype}")

                print("  Sample data:")
                print(df.head(2))
            else:
                print(f"  No data collected for {site_name}")

        except requests.exceptions.RequestException as e:
            print(f"Error processing {site_name}: {e}")
            continue
        except Exception as e:
            print(f"Unexpected error processing {site_name}: {e}")
            continue

    return results

def get_cds_era5_data(sites_df, start_year=1979, end_year=None):
    """Get climate data from Climate Data Store (CDS) ERA5 reanalysis"""
    # Import cdsapi
    import cdsapi

    if end_year is None:
        end_year = datetime.now().year

    # Create output directory
    output_dir = "/content/era5_data"
    os.makedirs(output_dir, exist_ok=True)

    results = {}

    # Initialize CDS client
    try:
        c = cdsapi.Client()
    except Exception as e:
        print(f"Error initializing CDS client: {e}")
        print("Please make sure your CDS API credentials are correctly set up.")
        return {}

    # Process each site
    for i, site in sites_df.iterrows():
        site_name = site['site_name']
        lat = site['latitude']
        lon = site['longitude']

        print(f"Processing site: {site_name} ({lat}, {lon})")

        # Define output file
        output_file = f"{output_dir}/{site_name}_era5.nc"

        # Prepare years and months for request
        years = [str(year) for year in range(start_year, end_year + 1)]
        months = [f"{month:02d}" for month in range(1, 13)]

        try:
            print(f"Requesting ERA5 data for {site_name}...")

            # Request temperature and precipitation data
            c.retrieve(
                'reanalysis-era5-single-levels-monthly-means',
                {
                    'product_type': 'monthly_averaged_reanalysis',
                    'variable': ['2m_temperature', 'total_precipitation'],
                    'year': years,
                    'month': months,
                    'time': '00:00',
                    'area': [lat+1, lon-1, lat-1, lon+1],  # N/W/S/E
                    'format': 'netcdf',
                },
                output_file)

            print(f"Data successfully downloaded to {output_file}")

            # Load the downloaded data
            try:
                ds = xr.open_dataset(output_file)
                # Extract data for the exact point
                ds = ds.sel(latitude=lat, longitude=lon, method='nearest')
                results[site_name] = ds
                print(f"Data loaded for {site_name}")
            except Exception as e:
                print(f"Error loading data for {site_name}: {e}")
                continue

        except Exception as e:
            print(f"Error retrieving data for {site_name}: {e}")
            continue

    return results

def get_noaa_climate_data(sites_df, start_year=1970, end_year=None, timeout=30):
    if end_year is None:
        end_year = datetime.now().year

    token = input("Enter your NOAA API token: ")
    # Correct base URL for NOAA's Climate Data Online (CDO) API
    base_url = "https://www.ncdc.noaa.gov/cdo-web/api/v2"
    headers = {'token': token}
    results = {}

    for i, site in sites_df.iterrows():
        site_name = site['site_name']
        lat = site['latitude']
        lon = site['longitude']

        print(f"Processing site: {site_name} ({lat}, {lon})")

        # First, find stations near the coordinates
        station_url = f"{base_url}/stations"
        params = {
            'extent': f"{lat-1},{lon-1},{lat+1},{lon+1}",
            'limit': 1000
        }

        try:
            # Adding timeout parameter to prevent hanging
            response = requests.get(station_url, headers=headers, params=params, timeout=timeout)
            response.raise_for_status()  # Raise exception for bad responses

            stations = response.json()
            if 'results' not in stations or len(stations['results']) == 0:
                print(f"No stations found near {site_name}")
                continue

            closest_station = min(
                stations['results'],
                key=lambda s: ((lat - float(s['latitude']))**2 + (lon - float(s['longitude']))**2)**0.5
            )
            station_id = closest_station['id']
            station_distance = ((lat - float(closest_station['latitude']))**2 +
                               (lon - float(closest_station['longitude']))**2)**0.5
            print(f"  Using station: {closest_station['name']} (ID: {station_id})")
            print(f"  Station coordinates: {closest_station['latitude']}, {closest_station['longitude']}")
            print(f"  Distance from site: {station_distance:.2f} degrees")

            # Create a container for this site's data
            site_data = []

            # Fetch data for each year (may need to do this in chunks due to API limitations)
            for year in range(start_year, end_year + 1):
                data_url = f"{base_url}/data"
                data_params = {
                    'datasetid': 'GHCND',  # Global Historical Climatology Network Daily
                    'stationid': station_id,
                    'startdate': f"{year}-01-01",
                    'enddate': f"{year}-12-31",
                    'datatypeid': 'PRCP, TMAX, TMIN, AWND, SNOW, SNWD, ACSH',
                    'units': 'metric',
                    'limit': 1000
                }

                try:
                    # Adding timeout parameter to prevent hanging
                    response = requests.get(data_url, headers=headers, params=data_params, timeout=timeout)
                    response.raise_for_status()

                    data = response.json()
                    if 'results' in data and data['results']:
                        print(f"  Year {year}: {len(data['results'])} records")
                        site_data.extend(data['results'])
                    else:
                        print(f"  Year {year}: No data found")

                except requests.exceptions.RequestException as e:
                    print(f"  Error fetching data for {site_name}, year {year}: {e}")
                    continue

            if site_data:
                # Convert to DataFrame for easier analysis
                df = pd.DataFrame(site_data)
                results[site_name] = df
                print(f"  Total records for {site_name}: {len(df)}")

                # Show a preview of the data structure
                print("  Data columns:")
                for col in df.columns:
                    print(f"    - {col}: {df[col].dtype}")

                print("  Sample data:")
                print(df.head(2))
            else:
                print(f"  No data collected for {site_name}")

        except requests.exceptions.RequestException as e:
            print(f"Error processing {site_name}: {e}")
            continue
        except Exception as e:
            print(f"Unexpected error processing {site_name}: {e}")
            continue

    return results

def get_cds_era5_data(sites_df, start_year=1979, end_year=None):
    """Get climate data from Climate Data Store (CDS) ERA5 reanalysis"""
    # Import cdsapi
    import cdsapi

    if end_year is None:
        end_year = datetime.now().year

    # Create output directory
    output_dir = "/content/era5_data"
    os.makedirs(output_dir, exist_ok=True)

    results = {}

    # Initialize CDS client
    try:
        c = cdsapi.Client()
    except Exception as e:
        print(f"Error initializing CDS client: {e}")
        print("Please make sure your CDS API credentials are correctly set up.")
        return {}

    # Process each site
    for i, site in sites_df.iterrows():
        site_name = site['site_name']
        lat = site['latitude']
        lon = site['longitude']

        print(f"Processing site: {site_name} ({lat}, {lon})")

        # Define output file
        output_file = f"{output_dir}/{site_name}_era5.nc"

        # Prepare years and months for request
        years = [str(year) for year in range(start_year, end_year + 1)]
        months = [f"{month:02d}" for month in range(1, 13)]

        try:
            print(f"Requesting ERA5 data for {site_name}...")

            # Request temperature and precipitation data
            c.retrieve(
                'reanalysis-era5-single-levels-monthly-means',
                {
                    'product_type': 'monthly_averaged_reanalysis',
                    'variable': ['2m_temperature', 'total_precipitation'],
                    'year': years,
                    'month': months,
                    'time': '00:00',
                    'area': [lat+1, lon-1, lat-1, lon+1],  # N/W/S/E
                    'format': 'netcdf',
                },
                output_file)

            print(f"Data successfully downloaded to {output_file}")

            # Load the downloaded data
            try:
                ds = xr.open_dataset(output_file)
                # Extract data for the exact point
                ds = ds.sel(latitude=lat, longitude=lon, method='nearest')
                results[site_name] = ds
                print(f"Data loaded for {site_name}")
            except Exception as e:
                print(f"Error loading data for {site_name}: {e}")
                continue

        except Exception as e:
            print(f"Error retrieving data for {site_name}: {e}")
            continue

    return results

def display_and_download_results(data_dict, output_dir="/content/climate_outputs"):
    """Process, display, and enable download of climate data results"""
    from IPython.display import display, HTML
    import os

    # Create local output directory first
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"Created local output directory: {output_dir}")

    # Process data for each site
    for site_name, data in data_dict.items():
        print(f"\n===== Processing {site_name} =====")

        # For NOAA data (pandas DataFrame)
        if isinstance(data, pd.DataFrame):
            # Display raw data sample
            print(f"\nNOAA Raw Data Sample for {site_name}:")
            display(data.head())

            # Save raw data locally first
            local_raw_file = f"{output_dir}/{site_name}_noaa_raw_data.csv"
            data.to_csv(local_raw_file, index=False)
            print(f"Raw data saved locally to: {local_raw_file}")

            # Process data if it has the expected structure
            if 'datatype' in data.columns and 'value' in data.columns and 'date' in data.columns:
                # Convert date to datetime
                data['date'] = pd.to_datetime(data['date'])
                data['year'] = data['date'].dt.year
                data['month'] = data['date'].dt.month

                # Define data types and their processing methods
                data_types = {
                    'PRCP': {'name': 'Precipitation', 'unit': 'mm', 'agg_func': 'sum'},
                    'TMAX': {'name': 'Maximum Temperature', 'unit': '\u00b0C', 'agg_func': 'mean'},
                    'TMIN': {'name': 'Minimum Temperature', 'unit': '\u00b0C', 'agg_func': 'mean'},
                    'AWND': {'name': 'Average Wind Speed', 'unit': 'm/s', 'agg_func': 'mean'},
                    'SNOW': {'name': 'Snowfall', 'unit': 'mm', 'agg_func': 'sum'},
                    'SNWD': {'name': 'Snow Depth', 'unit': 'mm', 'agg_func': 'mean'},
                    'ACSH': {'name': 'Average Cloudiness', 'unit': '%', 'agg_func': 'mean'}
                }

                # Process each data type
                for data_type, info in data_types.items():
                    data_subset = data[data['datatype'] == data_type].copy()
                    if not data_subset.empty:
                        annual_data = data_subset.groupby('year')['value'].agg(info['agg_func'])
                        data_df = pd.DataFrame({
                            'year': annual_data.index,
                            f"{info['name'].lower().replace(' ', '_')}": annual_data.values
                        })
                        print(f"\nAnnual {info['name']} for {site_name}:")
                        display(data_df)
                        csv_file = f"{output_dir}/{site_name}_{data_type.lower()}_annual.csv"
                        data_df.to_csv(csv_file, index=False)
                        print(f"Annual {info['name'].lower()} data saved to: {csv_file}")

            else:
                print(f"Warning: NOAA data for {site_name} doesn't have expected columns.")
                print(f"Available columns: {data.columns.tolist()}")

        # For CDS/ERA5 data (xarray Dataset)
        elif isinstance(data, xr.Dataset):
            print(f"\nERA5 Data Info for {site_name}:")
            print(data.info())

            # Save raw netCDF data
            local_nc_file = f"{output_dir}/{site_name}_era5_raw_data.nc"
            data.to_netcdf(local_nc_file)
            print(f"Raw ERA5 data saved locally to: {local_nc_file}")

            # Process temperature data (t2m in Kelvin)
            if 't2m' in data:
                # Convert from Kelvin to Celsius
                temp_data = data['t2m'] - 273.15

                # Calculate annual averages
                if 'time' in temp_data.dims:
                    # Extract year from time dimension
                    temp_data = temp_data.assign_coords(year=temp_data.time.dt.year)
                    annual_temp = temp_data.groupby('year').mean()

                    # Convert to DataFrame for display
                    temp_df = annual_temp.to_dataframe().reset_index()
                    print("\nAnnual Temperature Data:")
                    display(temp_df.head())

                    # Plot temperature trend
                    plt.figure(figsize=(12, 6))
                    plt.plot(annual_temp.year, annual_temp.values)
                    plt.title(f'Annual Average Temperature for {site_name} (1979-present)')
                    plt.xlabel('Year')
                    plt.ylabel('Temperature (\u00b0C)')
                    plt.grid(True)
                    temp_plot_file = f"{output_dir}/{site_name}_temperature_trend.png"
                    plt.savefig(temp_plot_file)
                    plt.show()
                    print(f"Temperature plot saved to: {temp_plot_file}")

                    # Save to CSV
                    temp_csv_file = f"{output_dir}/{site_name}_temperature_annual.csv"
                    temp_df.to_csv(temp_csv_file, index=False)
                    print(f"Annual temperature data saved to: {temp_csv_file}")

                    # Calculate trend
                    years = annual_temp.year.values
                    temps = annual_temp.values
                    slope, intercept, r_value, p_value, std_err = stats.linregress(years, temps)
                    print(f"Temperature trend: {slope:.4f}\u00b0C/year (p={p_value:.4f}, r²={r_value**2:.4f})")
                else:
                    print("Warning: Temperature data doesn't have a time dimension.")
            else:
                print("Warning: No temperature data ('t2m') found in ERA5 dataset.")

            # Process precipitation data (tp in meters)
            if 'tp' in data:
                # Convert from meters to mm (multiply by 1000)
                precip_data = data['tp'] * 1000

                # Calculate annual totals
                if 'time' in precip_data.dims:
                    # Extract year from time dimension
                    precip_data = precip_data.assign_coords(year=precip_data.time.dt.year)

                    # For monthly data, first sum by month then by year
                    # Multiply by the number of hours in the averaging period
                    # For monthly means, multiply by number of hours in each month
                    # This is a simplification - ideally we'd calculate exact hours per month
                    hours_in_month = 730.5  # Average hours in a month (365.25*24/12)
                    precip_data = precip_data * hours_in_month

                    annual_precip = precip_data.groupby('year').sum()

                    # Convert to DataFrame for display
                    precip_df = annual_precip.to_dataframe().reset_index()
                    print("\nAnnual Precipitation Data:")
                    display(precip_df.head())

                    # Plot precipitation trend
                    plt.figure(figsize=(12, 6))
                    plt.plot(annual_precip.year, annual_precip.values)
                    plt.title(f'Annual Total Precipitation for {site_name} (1979-present)')
                    plt.xlabel('Year')
                    plt.ylabel('Precipitation (mm)')
                    plt.grid(True)
                    prcp_plot_file = f"{output_dir}/{site_name}_precipitation_trend.png"
                    plt.savefig(prcp_plot_file)
                    plt.show()
                    print(f"Precipitation plot saved to: {prcp_plot_file}")

                    # Save to CSV
                    prcp_csv_file = f"{output_dir}/{site_name}_precipitation_annual.csv"
                    precip_df.to_csv(prcp_csv_file, index=False)
                    print(f"Annual precipitation data saved to: {prcp_csv_file}")

                    # Calculate trend
                    years = annual_precip.year.values
                    precips = annual_precip.values
                    slope, intercept, r_value, p_value, std_err = stats.linregress(years, precips)
                    print(f"Precipitation trend: {slope:.2f}mm/year (p={p_value:.4f}, r²={r_value**2:.4f})")
                else:
                    print("Warning: Precipitation data doesn't have a time dimension.")
            else:
                print("Warning: No precipitation data ('tp') found in ERA5 dataset.")

        else:
            print(f"No proper dataset available for {site_name}. Type: {type(data)}")

    # Offer downloads of all generated files
    print("\n\n===== FILES AVAILABLE FOR DOWNLOAD =====")
    file_list = os.listdir(output_dir)
    for i, filename in enumerate(file_list):
        print(f"{i+1}. {filename}")

    print("\nTo download files, run the following code in a new cell:")
    print("from google.colab import files")
    print(f"# Replace 'filename.csv' with desired file from the list above")
    print(f"files.download('{output_dir}/filename.csv')")

def create_station_info_csv(results, output_dir="/content/climate_outputs"):
    """Creates a CSV file with information about the weather station used for each glacier site."""
    station_data = []
    for site_name, data in results.items():
        if isinstance(data, pd.DataFrame) and 'station' in data.columns:
            station_id = data['station'].iloc[0]  # Assuming all records have the same station
            # Get station details from the first record (assuming it's representative)
            # Iterate through the rows of the DataFrame using iterrows()
            for index, row in data.iterrows():
                if row['station'] == station_id:
                    station_info = row.to_dict()  # Convert row to dictionary
                    print("station_info contents:", station_info)
                    break
            else:
                # If the loop completes without finding the station, set station_info to None
                station_info = None

            if station_info:
                station_data.append({
                    'site_name': site_name,
                    'station_id': station_id,
                    'station_name': station_info.get('name', 'N/A'),
                    'latitude': station_info.get('latitude', 'N/A'),
                    'longitude': station_info.get('longitude', 'N/A')
                })
            else:
                print(f"Warning: Station information not found for {site_name}")
        else:
            print(f"Warning: No station information available for {site_name}")
    if station_data:
        station_df = pd.DataFrame(station_data)  # station_df is defined here
        csv_file = f"{output_dir}/glacier_site_stations.csv"
        station_df.to_csv(csv_file, index=False)
        print(f"Station information saved to: {csv_file}")

        # Download the CSV file
        from google.colab import files
        files.download(csv_file)
    else:
        print("No station information collected.")
    return station_df  # Return station_df

# ... (rest of your code) ...

print("\nYour station sites:")
from IPython.display import display
station_df = create_station_info_csv(results) # call the function to create the dataframe
display(station_df)  # Now station_df is defined and can be displayed

# ... (rest of your code) ...

# Load glacier sites
print("==== CLIMATE DATA FOR GLACIER SITES (1979-present) ====")

print("\nStep 1: Upload your CSV file with glacier study sites")
# Define local output directory
output_dir = "/content/climate_outputs"
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# Option 1: Use the existing file path
file_path = "/content/drive/MyDrive/Diss/Climate_data_v6/glacier_locations.csv"

# Try to load from the predefined path first
try:
    sites_df = upload_glacier_sites(file_path)
    print("Successfully loaded glacier sites from Google Drive")
except FileNotFoundError:
    print("File not found. You can either:")
    print("1. Upload a file through Colab")
    print("2. Enter the file path manually")

    choice = input("Choose option (1 or 2): ")

    if choice == '1':
        print("Please upload your CSV file with glacier locations:")
        uploaded = files.upload()
        file_path = list(uploaded.keys())[0]
        sites_df = upload_glacier_sites(file_path)
    else:
        file_path = input("Enter the full path to your CSV file: ")
        sites_df = upload_glacier_sites(file_path)

print("\nYour glacier sites:")
from IPython.display import display
display(sites_df)

# Step 2: Call data fetching functions to get climate data (this should be before `create_station_info_csv`)
# ... [Code to choose data source and call get_noaa_climate_data or get_cds_era5_data here] ...
# ... [Example code:]
print("\nStep 2: Choose your climate data source")
print("1. NOAA Climate Data (requires API token)")
print("2. CDS ERA5 Data (requires CDS API credentials)")

choice = input("Enter your choice (1 or 2): ")

results = {} # Initialize results here

if choice == '1':
    print("\nAccessing NOAA Climate Data...")
    results = get_noaa_climate_data(sites_df) # Data is fetched here
else:
    print("\nAccessing CDS ERA5 Data...")
    if setup_cds_api():
        results = get_cds_era5_data(sites_df) # Data is fetched here
    else:
        print("Failed to configure CDS API. Please try again.")


print("\nYour station sites:")
from IPython.display import display
station_df = create_station_info_csv(results) # call the function to create the dataframe
display(station_df)  # Now station_df is defined and can be displayed

print("\nStep 2: Choose your climate data source")
print("1. NOAA Climate Data (requires API token)")
print("2. CDS ERA5 Data (requires CDS API credentials)")

choice = input("Enter your choice (1 or 2): ")

results = {}

if choice == '1':
    print("\nAccessing NOAA Climate Data...")
    results = get_noaa_climate_data(sites_df)
else:
    print("\nAccessing CDS ERA5 Data...")
    if setup_cds_api():
        results = get_cds_era5_data(sites_df)
    else:
        print("Failed to configure CDS API. Please try again.")

# Process and display the results
print("\nStep 3: Processing and displaying climate data")
if results:
    display_and_download_results(results, output_dir=output_dir)

    print("\nCreating zip file of all outputs for download...")
    # Define the 'create_zip_download' function before calling it
    import zipfile

    def create_zip_download(output_dir, zip_filename="climate_data_outputs.zip"):
        with zipfile.ZipFile(zip_filename, 'w') as zipf:
            for root, _, files in os.walk(output_dir):
                for file in files:
                    zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), output_dir))

        from google.colab import files
        files.download(zip_filename)

    create_zip_download(output_dir)
else:
    print("No data to process. Check previous errors.")

print("\nAnalysis complete!")
print(f"Files are saved in: {output_dir}")

# You can run this cell to download individual files
from google.colab import files

# Replace 'filename.csv' with the actual filename you want to download
files.download(f'{output_dir}/glacier_site_stations.csv')

# Or uncomment this to download everything as a zip file
# create_zip_download(output_dir)